{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77bd9c63-27ec-47cc-9a97-282040ad991e",
   "metadata": {},
   "source": [
    "# PaperBoat and API mini-hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6230ddb-4295-4e79-ad6e-165e9206c2ff",
   "metadata": {},
   "source": [
    "## Repo: https://github.com/lucafusarbassini/paperboat/tree/main\n",
    "## Author: Luca Fusar Bassini\n",
    "## Lausanne November 2023\n",
    "## LauzHack mini-hackathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0abc3d-7161-4d95-8d06-e1bb1486e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from flask import Flask, request, Response\n",
    "from scihub import SciHub\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from pyzotero import zotero\n",
    "from google.cloud import texttospeech as tts\n",
    "import wave\n",
    "import langchain\n",
    "from crossref.restful import Works\n",
    "from semanticscholar import SemanticScholar\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Sequence\n",
    "import openai\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from openai.embeddings_utils import get_embedding\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import langchain\n",
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import tool\n",
    "from datetime import date\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from semanticscholar import SemanticScholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4cabf8-6eef-471b-93e7-b1f9eb957b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: a few useful links\n",
    "# https://learn.deeplearning.ai/\n",
    "# https://platform.openai.com/docs/plugins/examples\n",
    "# https://python.langchain.com/docs/additional_resources/tutorials\n",
    "# https://github.com/openai/chatgpt-retrieval-plugin\n",
    "# https://cookbook.openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85e0b7f-1046-49db-8316-40cf8f8db1d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in all the API keys\n",
    "\n",
    "# Zotero\n",
    "try:\n",
    "    with open('zotero_key.txt', 'r') as file:\n",
    "        ZOTapi_key = file.read()\n",
    "except Exception as e:\n",
    "        print(e)\n",
    "try:\n",
    "    with open('zotero_id.txt', 'r') as file:\n",
    "        library_id = file.read()\n",
    "except Exception as e:\n",
    "        print(e)      \n",
    "library_type = 'user'\n",
    "zot = zotero.Zotero(library_id, library_type, ZOTapi_key)\n",
    "\n",
    "# OpenAI\n",
    "try:\n",
    "    with open('openai_key.txt', 'r') as file:\n",
    "        openai_key = file.read()\n",
    "except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# Telegram\n",
    "try:\n",
    "    with open('telegram_key.txt', 'r') as file:\n",
    "        TOKEN = file.read()\n",
    "except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# Zapier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292e83ff-8407-422c-8669-064f2a943ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = openai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d2f63-8d05-4e79-a6e0-d662365c306f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66be7ca6-a5a3-44ec-9444-a9a7de9d3c0e",
   "metadata": {},
   "source": [
    "## Scholar research APIs: CrossRef, Semantic Scholar, Zotero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b5ae9-ce29-4c1c-bb8c-3cd6964b6da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROSSREF: title to DOI\n",
    "\n",
    "def convert_title_to_doi(title):\n",
    "    url = f\"https://api.crossref.org/works?query.title={title}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        results = json.loads(response.content)['message']['items']\n",
    "        if len(results) > 0:\n",
    "            return results[0]['DOI']\n",
    "    return None\n",
    "\n",
    "print(convert_title_to_doi(\"RNA velocity of single cells\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1d32a-b927-4069-8787-fd2258a895a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZOTERO: adding to your library\n",
    "\n",
    "def add_to_zotero(title, doi):\n",
    "    template = zot.item_template('journalArticle')\n",
    "    template['DOI'] = doi\n",
    "    template['title'] = title\n",
    "    resp = zot.create_items([template])\n",
    "    return None\n",
    "\n",
    "add_to_zotero(\"RNA velocity of single cells\", \"10.1038/s41586-018-0414-6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2791e947-a608-43a9-9a9e-8f9c94f7fa80",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# semantic scholar - https://pypi.org/project/semanticscholar/\n",
    "\n",
    "# DOI <-> paper\n",
    "sch = SemanticScholar()\n",
    "paper = sch.get_paper('10.1038/s41586-018-0414-6')\n",
    "print(paper.title)\n",
    "\n",
    "# paper authors\n",
    "sch = SemanticScholar()\n",
    "author = sch.get_author(2262347)\n",
    "print(author.name)\n",
    "\n",
    "# list of papers\n",
    "sch = SemanticScholar()\n",
    "list_of_paper_ids = [\n",
    "     'CorpusId:470667',\n",
    "     '10.2139/ssrn.2250500',\n",
    "     '0f40b1f08821e22e859c6050916cec3667778613'\n",
    "]\n",
    "results = sch.get_papers(list_of_paper_ids)\n",
    "for item in results:\n",
    "     print(item.title)\n",
    "        \n",
    "# paper search by keyword\n",
    "sch = SemanticScholar()\n",
    "results = sch.search_paper('Computing Machinery and Intelligence')\n",
    "for item in results:\n",
    "     print(item.title)\n",
    "        \n",
    "# paper recommendations! - related to a given paper of interest\n",
    "sch = SemanticScholar()\n",
    "results = sch.get_recommended_papers('10.1145/3544585.3544600')\n",
    "for item in results:\n",
    "     print(item.title)\n",
    "        \n",
    "# using also NEGATIVE paper examples\n",
    "sch = SemanticScholar()\n",
    "positive_paper_ids = ['10.1145/3544585.3544600']\n",
    "negative_paper_ids = ['10.1145/301250.301271']\n",
    "results = sch.get_recommended_papers_from_lists(positive_paper_ids, negative_paper_ids)\n",
    "for item in results:\n",
    "     print(item.title)\n",
    "        \n",
    "# retrieving the categories (fields) of papers\n",
    "sch = SemanticScholar()\n",
    "results = sch.search_paper('software engineering', fields_of_study=['Computer Science','Education'])\n",
    "print(results[0].s2FieldsOfStudy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8325d3-8bd5-47e6-9d02-1a587b54abec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6ae6ff6-ec08-427b-bba2-18c258b30ee7",
   "metadata": {},
   "source": [
    "# Scihub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da582c4-dda2-4cc1-af22-05aa4f3313b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scihub import SciHub\n",
    "import random\n",
    "\n",
    "random_number = str(random.randint(1, 100000))\n",
    "sh = SciHub()\n",
    "result = sh.download(\"10.1038/s41586-018-0414-6\", path=\"papero\"+random_number+\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1118c32d-732b-45da-b042-e9c0deb14043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16d67e6a-6480-4516-8efc-bd3e1ad431b7",
   "metadata": {},
   "source": [
    "## Text-to-voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f49c7-9a5e-43e4-98de-8c6fc1091869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_languages_from_voices(voices: Sequence[tts.Voice]):\n",
    "    language_set = set()\n",
    "    for voice in voices:\n",
    "        for language_code in voice.language_codes:\n",
    "            language_set.add(language_code)\n",
    "    return language_set\n",
    "\n",
    "def list_languages():\n",
    "    client = tts.TextToSpeechClient()\n",
    "    response = client.list_voices()\n",
    "    languages = unique_languages_from_voices(response.voices)\n",
    "\n",
    "    print(f\" Languages: {len(languages)} \".center(60, \"-\"))\n",
    "    for i, language in enumerate(sorted(languages)):\n",
    "        print(f\"{language:>10}\", end=\"\\n\" if i % 5 == 4 else \"\")\n",
    "\n",
    "def list_voices(language_code=None):\n",
    "    client = tts.TextToSpeechClient()\n",
    "    response = client.list_voices(language_code=language_code)\n",
    "    voices = sorted(response.voices, key=lambda voice: voice.name)\n",
    "\n",
    "    print(f\" Voices: {len(voices)} \".center(60, \"-\"))\n",
    "    for voice in voices:\n",
    "        languages = \", \".join(voice.language_codes)\n",
    "        name = voice.name\n",
    "        gender = tts.SsmlVoiceGender(voice.ssml_gender).name\n",
    "        rate = voice.natural_sample_rate_hertz\n",
    "        print(f\"{languages:<8} | {name:<24} | {gender:<8} | {rate:,} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e97658-b4a5-4b5c-9911-483d2c4c3b7b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# installation for authentication a bit cumbersome\n",
    "# https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev\n",
    "# https://cloud.google.com/sdk/docs/install\n",
    "\n",
    "# gcloud auth application-default login\n",
    "# gcloud auth application-default set-quota-project YOUR_PROJECT\n",
    "\n",
    "def text_to_wav(voice_name: str, text: str):\n",
    "    language_code = \"-\".join(voice_name.split(\"-\")[:2])\n",
    "    text_input = tts.SynthesisInput(text=text)\n",
    "    voice_params = tts.VoiceSelectionParams(\n",
    "        language_code=language_code, name=voice_name\n",
    "    )\n",
    "    audio_config = tts.AudioConfig(audio_encoding=tts.AudioEncoding.LINEAR16)\n",
    "\n",
    "    client = tts.TextToSpeechClient()\n",
    "    response = client.synthesize_speech(\n",
    "        input=text_input,\n",
    "        voice=voice_params,\n",
    "        audio_config=audio_config,\n",
    "    )\n",
    "\n",
    "    filename = f\"{voice_name}.wav\"\n",
    "    with open(filename, \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "        print(f'Generated speech saved to \"{filename}\"')\n",
    "        \n",
    "text_to_wav(\"en-US-Neural2-H\", \"This highly informative series of exclusive scientific sessions will feature keynote presentations by leading researchers, expert panels, and networking opportunities with peers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cb7cbd-ebbe-41da-ab66-76760a48156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_file_name = 'en-US-Neural2-H.wav'\n",
    "new_file_name = 'comb.wav'\n",
    "os.rename(original_file_name, new_file_name)\n",
    "        \n",
    "for j in range(0,len(week_summaries)):\n",
    "    try:\n",
    "        text_to_wav(\"en-US-Neural2-H\", week_summaries[j])\n",
    "        with wave.open('comb.wav', 'rb') as wav1:\n",
    "            with wave.open('en-US-Neural2-H.wav', 'rb') as wav2:\n",
    "                params1 = wav1.getparams()\n",
    "                params2 = wav2.getparams()\n",
    "                with wave.open('combined.wav', 'wb') as wav_combined:\n",
    "                    wav_combined.setparams(params1)\n",
    "                    data = wav1.readframes(wav1.getnframes())\n",
    "                    wav_combined.writeframes(data)\n",
    "                    data = wav2.readframes(wav2.getnframes())\n",
    "                    wav_combined.writeframes(data)\n",
    "        os.rename(\"combined.wav\", \"comb.wav\")\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a0377e-bfa8-4106-b377-5f1513f2bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHUNK_SIZE = 48 * 1024 * 1024  # 50 MB in bytes\n",
    "\n",
    "# Open the input file for reading\n",
    "with wave.open('comb.wav', 'rb') as input_file:\n",
    "    # Determine the number of output files needed\n",
    "    num_output_files = math.ceil(input_file.getnframes() * input_file.getsampwidth() / MAX_CHUNK_SIZE)\n",
    "\n",
    "    # Split the input file into smaller chunks and write each chunk to a separate output file\n",
    "    for i in range(num_output_files):\n",
    "        output_file_name = f'comb{i+1}.wav'\n",
    "        with wave.open(output_file_name, 'wb') as output_file:\n",
    "            # Set the parameters of the output file to match the input file\n",
    "            output_file.setnchannels(input_file.getnchannels())\n",
    "            output_file.setsampwidth(input_file.getsampwidth())\n",
    "            output_file.setframerate(input_file.getframerate())\n",
    "\n",
    "            # Determine the number of frames to read from the input file\n",
    "            num_frames = min(MAX_CHUNK_SIZE // input_file.getsampwidth(), input_file.getnframes() - input_file.tell())\n",
    "\n",
    "            # Read the frames from the input file and write them to the output file\n",
    "            frames = input_file.readframes(num_frames)\n",
    "            output_file.writeframes(frames)\n",
    "\n",
    "# Print the total size of the output files\n",
    "total_output_size = sum(os.path.getsize(f'comb{i+1}.wav') for i in range(num_output_files))\n",
    "print(f'Total size of output files: {total_output_size / (1024 * 1024):.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f6e8e-9c87-453b-8c4c-39b07d1d1626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative to Google - which probably requires google cloud... messy!\n",
    "\n",
    "import rlvoice\n",
    "import gtts\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from playsound import playsound\n",
    "import time\n",
    "\n",
    "\n",
    "text = \"Leonardo da Vinci lived in France for ten years.\"\n",
    "n_trials = 1\n",
    "\n",
    "# # -- offline\n",
    "# start = time.time()\n",
    "# for _ in range(n_trials):\n",
    "#     engine = rlvoice.init()\n",
    "#     engine.save_to_file(text, \"rlvoice.mp3\")\n",
    "#     engine.runAndWait()\n",
    "# avg_time = (time.time() - start) / n_trials\n",
    "# print(f\"Average time for rlvoice: {avg_time:.2f} s\")\n",
    "\n",
    "# engine.say(text)\n",
    "# engine.runAndWait()\n",
    "\n",
    "# -- online (Google)\n",
    "start_time = time.time()\n",
    "for _ in range(n_trials):\n",
    "    tts = gtts.gTTS(text, tld=\"com\", lang=\"en\", slow=False)\n",
    "    tts.save(\"gtts.mp3\")\n",
    "avg_time = (time.time() - start_time) / n_trials\n",
    "print(f\"Average time for gtts: {avg_time:.2f} s\")\n",
    "# playsound(\"test.mp3\")\n",
    "\n",
    "# -- hugging face (offline)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load the processor\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "# load the model\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\").to(device)\n",
    "# load the vocoder, that is the voice encoder\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\").to(device)\n",
    "# we load this dataset to get the speaker embeddings\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "\n",
    "# speaker ids from the embeddings dataset\n",
    "speakers = {\n",
    "    'awb': 0,     # Scottish male\n",
    "    'bdl': 1138,  # US male\n",
    "    'clb': 2271,  # US female\n",
    "    'jmk': 3403,  # Canadian male\n",
    "    'ksp': 4535,  # Indian male\n",
    "    'rms': 5667,  # US male\n",
    "    'slt': 6799   # US female\n",
    "}\n",
    "\n",
    "def save_text_to_speech(text, speaker=\"rms\", output_filename=None):\n",
    "\n",
    "    if output_filename is None:\n",
    "        output_filename = f\"{speaker}-{'-'.join(text.split()[:6])}.mp3\"\n",
    "\n",
    "    # preprocess text\n",
    "    inputs = processor(text=text, return_tensors=\"pt\").to(device)\n",
    "    if speaker is not None:\n",
    "        # load xvector containing speaker's voice characteristics from a dataset\n",
    "        speaker_embeddings = torch.tensor(embeddings_dataset[speaker][\"xvector\"]).unsqueeze(0).to(device)\n",
    "    else:\n",
    "        # random vector, meaning a random voice\n",
    "        speaker_embeddings = torch.randn((1, 512)).to(device)\n",
    "    # generate speech with the models\n",
    "    speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "\n",
    "    # save the generated speech to a file with 16KHz sampling rate\n",
    "    sf.write(output_filename, speech.cpu().numpy(), samplerate=16000)\n",
    "\n",
    "    # return the filename for reference\n",
    "    return output_filename\n",
    "\n",
    "\n",
    "# generate speech with a US female voice\n",
    "start = time.time()\n",
    "for _ in range(n_trials):\n",
    "    save_text_to_speech(text, speaker=speakers[\"bdl\"], output_filename=\"huggingface.mp3\")\n",
    "avg_time = (time.time() - start) / n_trials\n",
    "print(f\"Average time for huggingface: {avg_time:.2f} s\")\n",
    "\n",
    "# playsound(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2a4d4e-c00a-439a-856d-b01270eff104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fdb3ccc-c19d-47d2-8c12-1be3336fa521",
   "metadata": {},
   "source": [
    "## Agents: ChatGPT + LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e838d8-3567-4326-80fd-29eb8613560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain: agents that can search the web\n",
    "llm_model = \"gpt-3.5-turbo\"\n",
    "\n",
    "# create an instance of ChatGPT\n",
    "llm = ChatOpenAI(temperature=0, model=llm_model, openai_api_key=mykey)\n",
    "\n",
    "# parse useful tools, eg, Wikipedia, that the \"agent\" can use\n",
    "tools = load_tools([\"llm-math\",\"wikipedia\"], llm=llm)\n",
    "\n",
    "agent= initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be95855f-4c24-4d2a-b206-7a92a35a9a06",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent(\"What is the 25% of 300?\") # look how cool is the behind the scenes to build new tools..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5110f98f-9452-4895-8283-40ecda281c06",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Tom M. Mitchell is an American computer scientist \\\n",
    "and the Founders University Professor at Carnegie Mellon University (CMU)\\\n",
    "what book did he write?\"\n",
    "result = agent(question) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bbef5c-1cf9-421e-ad63-14982735b556",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make an agent that can write and EXECUTE inside it freshly-written, dynamic Python code!\n",
    "agent = create_python_agent(\n",
    "    llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "customer_list = [[\"Harrison\", \"Chase\"], \n",
    "                 [\"Lang\", \"Chain\"],\n",
    "                 [\"Dolly\", \"Too\"],\n",
    "                 [\"Elle\", \"Elem\"], \n",
    "                 [\"Geoff\",\"Fusion\"], \n",
    "                 [\"Trance\",\"Former\"],\n",
    "                 [\"Jen\",\"Ayai\"]\n",
    "                ]\n",
    "\n",
    "agent.run(f\"\"\"Sort these customers by \\\n",
    "last name and then first name \\\n",
    "and print the output: {customer_list}\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90560fd4-4594-4be3-97e5-156f3a884872",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inspect the full behind the scenes\n",
    "langchain.debug=True\n",
    "agent.run(f\"\"\"Sort these customers by \\\n",
    "last name and then first name \\\n",
    "and print the output: {customer_list}\"\"\") \n",
    "langchain.debug=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dd3141-5699-48a8-8540-88fbc688442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building langchain agents/tools with prompting\n",
    "\n",
    "@tool\n",
    "def time(text: str) -> str:\n",
    "    \"\"\"Returns todays date, use this for any \\\n",
    "    questions related to knowing todays date. \\\n",
    "    The input should always be an empty string, \\\n",
    "    and this function will always return todays \\\n",
    "    date - any date mathmatics should occur \\\n",
    "    outside this function.\"\"\"\n",
    "    return str(date.today())\n",
    "\n",
    "agent= initialize_agent(\n",
    "    tools + [time], \n",
    "    llm, \n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose = True)\n",
    "\n",
    "try:\n",
    "    result = agent(\"whats the date today?\") \n",
    "except: \n",
    "    print(\"exception on external access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f453a27b-a922-4dca-ad6d-739b8152545e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbdd8337-daa9-49a4-88dc-315eed9d0e12",
   "metadata": {},
   "source": [
    "## Using directly the ChatGPT API + web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86feb0fa-af44-4fff-9d7a-ba22ef8f55b9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chatgpt + scraping \n",
    "today = datetime.date.today() - datetime.timedelta(days=3) # exmaple: for biorxiv, three days ago, as DOI updates on biorxiv with about 3 days of delay\n",
    "print(today)\n",
    "\n",
    "# make a scraping url adding the variable info, here the date\n",
    "url = str(\"https://www.biorxiv.org/search/jcode%3Amedrxiv%7C%7Cbiorxiv%20limit_from%3A\"+str(today)+\"%20limit_to%3A\"+str(today)+\"%20numresults%3A1000%20sort%3Arelevance-rank%20format_result%3Astandard\")\n",
    "\n",
    "# scrape with requests\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    html_content = response.content\n",
    "else:\n",
    "    print(\"Error retrieving HTML content:\", response.status_code)\n",
    "\n",
    "# parse the HTML using Beautiful Soup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96bcd77-fae5-4c59-af80-ea03700a9f79",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find all DOI links in the HTML using a regular expression - of course ChatGPT made the regex for me\n",
    "doi_regex = re.compile(r\"https?://doi\\.org/[^\\s]+\")\n",
    "doi_links = soup.find_all(text=doi_regex)\n",
    "\n",
    "doi_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab2894-c4a7-453d-a54a-d2a2ea36a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt):\n",
    "    messag=[{\"role\": \"system\", \"content\": \"You are a chatbot\"}]\n",
    "    \n",
    "    ## build a chat history: you can CONDITION the bot on the style of replies you want to see - also getting weird behaviors... such as KanyeGPT\n",
    "    history_bot = [\"Yes, I'm ready! Please provide the first paper abstract.\"]\n",
    "    \n",
    "    # ask ChatGPT to return STRUCTURED, parsable answers that you can extract easily - often better providing examples of desired behavior (1-2 example often enough)\n",
    "    history_user = [\"i'll give you some paper abstracts. for each abstract (i.e., for each of my messages), you will a) assign a topic from the following list:\\nbiochemistry\\nbiophysics\\nproteomics\\ncancer\\ncell biology\\nmolecular and synthetic biology\\ncomputational biology\\ngenetics and genomics\\npathology\\nimmunology\\nmicrobiology\\nneuroscience\\ndevelopmental biology\\nethology and behavior\\nzoology\\nplant biology\\nindustrial biotechnology\\npharmacology\\nengineering\\nvirology\\nmachine learning\\nchemical biology\\nnanomedicine\\naging\\necology and evolution\\nvaccinology\\nepidemiology\\nclinical trials,\\nb) write a 2-sentences summary, focusing on the key innovation presented in that abstract.\\n\\nfor example:\\nmy input = The spontaneous deamination of cytosine is a major source of transitions from C•G to T•A base pairs, which account for half of known pathogenic point mutations in humans. The ability to efficiently convert targeted A•T base pairs to G•C could therefore advance the study and treatment of genetic diseases. The deamination of adenine yields inosine, which is treated as guanine by polymerases, but no enzymes are known to deaminate adenine in DNA. Here we describe adenine base editors (ABEs) that mediate the conversion of A•T to G•C in genomic DNA. We evolved a transfer RNA adenosine deaminase to operate on DNA when fused to a catalytically impaired CRISPR–Cas9 mutant. Extensive directed evolution and protein engineering resulted in seventh-generation ABEs that convert targeted A•T base pairs efficiently to G•C (approximately 50% efficiency in human cells) with high product purity (typically at least 99.9%) and low rates of indels (typically no more than 0.1%). ABEs introduce point mutations more efficiently and cleanly, and with less off-target genome modification, than a current Cas9 nuclease-based method, and can install disease-correcting or disease-suppressing mutations in human cells. Together with previous base editors, ABEs enable the direct, programmable introduction of all four transition mutations without double-stranded DNA cleavage.\\n\\nyour output =\\na. genetics and genomics\\nb. A new base-editor that converts A-T to G-C, based on an RNA adenosine deaminase fused to catalitically-impaired CRISPR-Cas9. Base editors can install therapeutic mutations in genomic DNA in human cells with no double-strand break.\\nready to start?\"]\n",
    "    \n",
    "    for user_message, bot_message in zip(history_user, history_bot):\n",
    "        messag.append({\"role\": \"user\", \"content\": str(user_message)})\n",
    "        messag.append({\"role\": \"system\", \"content\": str(bot_message)})\n",
    "    messag.append({\"role\": \"user\", \"content\": str(prompt)})\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        \n",
    "    # please use gtp3.5 although gpt4 is much better for $$\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "        messages=messag\n",
    "    )\n",
    "    result = ''\n",
    "    for choice in response.choices:\n",
    "        result += choice.message.content\n",
    "    history_bot.append(result)\n",
    "    history_user.append(str(prompt))\n",
    "    return result\n",
    "\n",
    "print(generate_response(\"The power of human language and thought arises from systematic compositionality—the algebraic ability to understand and produce novel combinations from known components. Fodor and Pylyshyn1 famously argued that artificial neural networks lack this capacity and are therefore not viable models of the mind. Neural networks have advanced considerably in the years since, yet the systematicity challenge persists. Here we successfully address Fodor and Pylyshyn’s challenge by providing evidence that neural networks can achieve human-like systematicity when optimized for their compositional skills. To do so, we introduce the meta-learning for compositionality (MLC) approach for guiding training through a dynamic stream of compositional tasks. To compare humans and machines, we conducted human behavioural experiments using an instruction learning paradigm. After considering seven different models, we found that, in contrast to perfectly systematic but rigid probabilistic symbolic models, and perfectly flexible but unsystematic neural networks, only MLC achieves both the systematicity and flexibility needed for human-like generalization. MLC also advances the compositional skills of machine learning systems in several systematic generalization benchmarks. Our results show how a standard neural network architecture, optimized for its compositional skills, can mimic human systematic generalization in a head-to-head comparison.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e3087-f63e-44f9-90fc-f6b4ff293cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trivial web scraping example - note, it's slow\n",
    "\n",
    "titles = []\n",
    "abstracts = []\n",
    "\n",
    "for link in doi_links[0:3]:\n",
    "    print(link)\n",
    "    response = requests.get(link)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        html_content = response.content\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # find the div element with class=\"section abstract\" and id=\"abstract-1\"\n",
    "        abstract_div = soup.find(\"div\", {\"class\": \"section abstract\", \"id\": \"abstract-1\"})\n",
    "\n",
    "        # extract the text from the first p element inside the abstract_div\n",
    "        abstract_text = abstract_div.find(\"p\").text.strip()\n",
    "\n",
    "        # clean up the extracted text\n",
    "        abstract_text = abstract_text.strip()\n",
    "        abstract_text = abstract_text.replace(\"\\n\", \" \")\n",
    "\n",
    "        div_tag = soup.find('div', {'class': 'highwire-cite highwire-cite-highwire-article highwire-citation-biorxiv-article-pap-list clearfix'})\n",
    "        title = div_tag.find('div', {'class': 'highwire-cite-title'}).text.strip()\n",
    "        \n",
    "        titles.append(title)\n",
    "        abstracts.append(abstract_text)\n",
    "        \n",
    "    else:\n",
    "        print(\"Error retrieving HTML content:\", response.status_code)\n",
    "        \n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c1e431-dc40-49ea-86a0-ee48acae7019",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"72\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)\n",
    "\n",
    "def run_conversation():\n",
    "    # Step 1: send the conversation and available functions to GPT\n",
    "    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}]\n",
    "    functions = [\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        messages=messages,\n",
    "        functions=functions,\n",
    "        function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"]\n",
    "\n",
    "    # Step 2: check if GPT wanted to call a function\n",
    "    if response_message.get(\"function_call\"):\n",
    "        # Step 3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "        available_functions = {\n",
    "            \"get_current_weather\": get_current_weather,\n",
    "        }  # only one function in this example, but you can have multiple\n",
    "        function_name = response_message[\"function_call\"][\"name\"]\n",
    "        function_to_call = available_functions[function_name]\n",
    "        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
    "        function_response = function_to_call(\n",
    "            location=function_args.get(\"location\"),\n",
    "            unit=function_args.get(\"unit\"),\n",
    "        )\n",
    "\n",
    "        # Step 4: send the info on the function call and function response to GPT\n",
    "        messages.append(response_message)  # extend conversation with assistant's reply\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response,\n",
    "            }\n",
    "        )  # extend conversation with function response\n",
    "        second_response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-0613\",\n",
    "            messages=messages,\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "        return second_response\n",
    "\n",
    "print(run_conversation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5455613c-e510-41ef-bf0a-8a6d260d0e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a92fe874-ab1c-4e0c-ac68-7ec0ae7437f5",
   "metadata": {},
   "source": [
    "## Structuring ChatGPT's \"reasoning chain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf786d36-02be-4f46-aada-b2ca6794a0f6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"chain-of-thought\" prompting: structuring LLM's activity to resemble human reasoning at structured tasks involving language\n",
    "# note how you can use prompting creatively\n",
    "\n",
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, max_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens, \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "delimiter = \"####\"\n",
    "system_message = f\"\"\"\n",
    "Follow these steps to answer the customer queries.\n",
    "The customer query will be delimited with four hashtags,\\\n",
    "i.e. {delimiter}. \n",
    "\n",
    "Step 1:{delimiter} First decide whether the user is \\\n",
    "asking a question about a specific product or products. \\\n",
    "Product cateogry doesn't count. \n",
    "\n",
    "Step 2:{delimiter} If the user is asking about \\\n",
    "specific products, identify whether \\\n",
    "the products are in the following list.\n",
    "All available products: \n",
    "1. Product: TechPro Ultrabook\n",
    "   Category: Computers and Laptops\n",
    "   Brand: TechPro\n",
    "   Model Number: TP-UB100\n",
    "   Warranty: 1 year\n",
    "   Rating: 4.5\n",
    "   Features: 13.3-inch display, 8GB RAM, 256GB SSD, Intel Core i5 processor\n",
    "   Description: A sleek and lightweight ultrabook for everyday use.\n",
    "   Price: $799.99\n",
    "\n",
    "2. Product: BlueWave Gaming Laptop\n",
    "   Category: Computers and Laptops\n",
    "   Brand: BlueWave\n",
    "   Model Number: BW-GL200\n",
    "   Warranty: 2 years\n",
    "   Rating: 4.7\n",
    "   Features: 15.6-inch display, 16GB RAM, 512GB SSD, NVIDIA GeForce RTX 3060\n",
    "   Description: A high-performance gaming laptop for an immersive experience.\n",
    "   Price: $1199.99\n",
    "\n",
    "3. Product: PowerLite Convertible\n",
    "   Category: Computers and Laptops\n",
    "   Brand: PowerLite\n",
    "   Model Number: PL-CV300\n",
    "   Warranty: 1 year\n",
    "   Rating: 4.3\n",
    "   Features: 14-inch touchscreen, 8GB RAM, 256GB SSD, 360-degree hinge\n",
    "   Description: A versatile convertible laptop with a responsive touchscreen.\n",
    "   Price: $699.99\n",
    "\n",
    "4. Product: TechPro Desktop\n",
    "   Category: Computers and Laptops\n",
    "   Brand: TechPro\n",
    "   Model Number: TP-DT500\n",
    "   Warranty: 1 year\n",
    "   Rating: 4.4\n",
    "   Features: Intel Core i7 processor, 16GB RAM, 1TB HDD, NVIDIA GeForce GTX 1660\n",
    "   Description: A powerful desktop computer for work and play.\n",
    "   Price: $999.99\n",
    "\n",
    "5. Product: BlueWave Chromebook\n",
    "   Category: Computers and Laptops\n",
    "   Brand: BlueWave\n",
    "   Model Number: BW-CB100\n",
    "   Warranty: 1 year\n",
    "   Rating: 4.1\n",
    "   Features: 11.6-inch display, 4GB RAM, 32GB eMMC, Chrome OS\n",
    "   Description: A compact and affordable Chromebook for everyday tasks.\n",
    "   Price: $249.99\n",
    "\n",
    "Step 3:{delimiter} If the message contains products \\\n",
    "in the list above, list any assumptions that the \\\n",
    "user is making in their \\\n",
    "message e.g. that Laptop X is bigger than \\\n",
    "Laptop Y, or that Laptop Z has a 2 year warranty.\n",
    "\n",
    "Step 4:{delimiter}: If the user made any assumptions, \\\n",
    "figure out whether the assumption is true based on your \\\n",
    "product information. \n",
    "\n",
    "Step 5:{delimiter}: First, politely correct the \\\n",
    "customer's incorrect assumptions if applicable. \\\n",
    "Only mention or reference products in the list of \\\n",
    "5 available products, as these are the only 5 \\\n",
    "products that the store sells. \\\n",
    "Answer the customer in a friendly tone.\n",
    "\n",
    "Use the following format:\n",
    "Step 1:{delimiter} <step 1 reasoning>\n",
    "Step 2:{delimiter} <step 2 reasoning>\n",
    "Step 3:{delimiter} <step 3 reasoning>\n",
    "Step 4:{delimiter} <step 4 reasoning>\n",
    "Response to user:{delimiter} <response to customer>\n",
    "\n",
    "Make sure to include {delimiter} to separate every step.\n",
    "\"\"\"\n",
    "\n",
    "# again note the steps the \"chain\"\n",
    "user_message = f\"\"\"\n",
    "by how much is the BlueWave Chromebook more expensive \\\n",
    "than the TechPro Desktop\"\"\"\n",
    "\n",
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content': system_message},    \n",
    "{'role':'user', \n",
    " 'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "] \n",
    "\n",
    "response = get_completion_from_messages(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393efc6d-6655-4098-bb1c-159b67f68a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e275c3b-b148-4734-b838-deed1254c419",
   "metadata": {},
   "source": [
    "## OpenAI vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce3cbbf-9d20-43f2-bd52-cfc57bc35ebf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ChatGPT vector embeddings\n",
    "\n",
    "# embedding model parameters\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "max_tokens = 8000  # the maximum for text-embedding-ada-002 is 8191\n",
    "\n",
    "# load & inspect dataset\n",
    "input_datapath = \"Reviews.csv\" \n",
    "df = pd.read_csv(input_datapath, index_col=0)\n",
    "df = df[[\"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]]\n",
    "df = df.dropna()\n",
    "df[\"combined\"] = (\n",
    "    \"Title: \" + df.Summary.str.strip() + \"; Content: \" + df.Text.str.strip()\n",
    ")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15694c62-525b-49d1-a82d-81833e6bce08",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# subsample to 1k most recent reviews and remove samples that are too long\n",
    "top_n = 1000\n",
    "df = df.sort_values(\"Time\").tail(top_n * 2)  # first cut to first 2k entries, assuming less than half will be filtered out\n",
    "df.drop(\"Time\", axis=1, inplace=True)\n",
    "\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "\n",
    "# omit reviews that are too long to embed\n",
    "df[\"n_tokens\"] = df.combined.apply(lambda x: len(encoding.encode(x)))\n",
    "df = df[df.n_tokens <= max_tokens].tail(top_n)\n",
    "\n",
    "df = df.iloc[1:20,]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d27df7-1fd9-4159-8247-43c07cafb138",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this may take a few minutes\n",
    "df[\"embedding\"] = df.combined.apply(lambda x: get_embedding(x, engine=embedding_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77114ec-0c69-458f-8cd4-ad79fae56329",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"fine_food_reviews_with_embeddings_1k.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c1b30-764f-4375-b84e-95164d92f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how long are the review embeddings?\n",
    "# we could now treat each review as a 1536-features element of a vector space to do clusterings etc!!\n",
    "# much more here: https://platform.openai.com/docs/guides/embeddings/use-cases\n",
    "\n",
    "print(len(df[\"embedding\"].iloc[1,]))\n",
    "print(len(df[\"embedding\"].iloc[17,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a35b04-cc89-4756-94c1-d580c594390b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19e3ea9d-5e1e-49e0-8e51-c4e19980a3a6",
   "metadata": {},
   "source": [
    "## Other APIs, eg DALL-E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f0e6a-a524-4857-b094-1447d3fd3316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wanna really get a cliché image generated right here in the flow of your python script?\n",
    "response = openai.Image.create(\n",
    "  prompt=\"a white siamese cat\",\n",
    "  n=1,\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "image_url = response['data'][0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9a5bb-6da2-4e34-8647-1687b39630ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url # https://platform.openai.com/docs/guides/images/introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed290540-ed51-42ea-9dba-1fa80a944638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2655146d-c5ea-440a-95f2-df2a8ec19702",
   "metadata": {},
   "source": [
    "# Search over pdf documents with LangChain + others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1737fd32-ec26-4f1a-bc9c-826857f9c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone\n",
    "\n",
    "#loader = UnstructuredPDFLoader(\"field-guide-to-data-science.pdf\")\n",
    "loader = OnlinePDFLoader(\"https://www.biorxiv.org/content/10.1101/2023.07.20.549945v1.full.pdf\")\n",
    "\n",
    "data = loader.load_and_split() \n",
    "\n",
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[0].page_content)} characters in your document')\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(data)\n",
    "\n",
    "print (f'Now you have {len(texts)} documents')\n",
    "\n",
    "OPENAI_API_KEY = mykey\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "import pinecone      \n",
    "\n",
    "pinecone.init(      \n",
    "\tapi_key=mykey,      \n",
    "\tenvironment='gcp-starter'      \n",
    ")      \n",
    "index = pinecone.Index('langchain')\n",
    "\n",
    "index_name = \"langchain\"  ####### I0VE STARTED A VERY RANDOM INDEX ON FREE TIER IN PINECONE!!\n",
    "\n",
    "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)\n",
    "\n",
    "query = \"what is Voyager?\"\n",
    "docs = docsearch.similarity_search(query)#, include_metadata=True)\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cf55aa-4c05-472b-9d93-8d93210fb3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY) ### temperature parameter controls the \"creativity\" (aka, hallucinations)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "query = \"summarize the technical improvements in Voyager\"\n",
    "docs = docsearch.similarity_search(query)#, include_metadata=True)\n",
    "\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b97c91-c64f-4875-8d1a-a822ec6d2f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = mykey\n",
    "os.environ[\"SERPAPI_API_KEY\"] = mykey\n",
    "\n",
    "# provide the path of  pdf file/files.\n",
    "pdfreader = PdfReader('Document sans titre.pdf')\n",
    "\n",
    "from typing_extensions import Concatenate\n",
    "# read text from pdf\n",
    "raw_text = ''\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content\n",
    "        \n",
    "# We need to split the text using Character Text Split such that it sshould not increse token size\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)\n",
    "\n",
    "# Download embeddings from OpenAI\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "document_search = FAISS.from_texts(texts, embeddings)\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")\n",
    "\n",
    "query = \"networks?\"\n",
    "docs = document_search.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)\n",
    "\n",
    "query = \"How much the agriculture target will be increased to and what the focus will be\"\n",
    "docs = document_search.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)\n",
    "\n",
    "from langchain.document_loaders import OnlinePDFLoader\n",
    "\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/1706.03762.pdf\")\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "# Download embeddings from OpenAI\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "\n",
    "query = \"Explain me about Attention is all you need\"\n",
    "index.query(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
